name: Evaluate

on:
  push:
    branches: [ main, develop ]
    paths: 
      - 'src/**'
      - 'prompts/**'
      - 'requirements.txt'
      - 'tests/evaluation/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'prompts/**'
      - 'requirements.txt'
      - 'tests/evaluation/**'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.12'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # NOTE: Additional evaluation packages now included in requirements.txt with beta versions

    - name: Verify test documents
      run: |
        echo "Checking test document structure..."
        ls -la tests/evaluation/test_documents/
        if [ -d "tests/evaluation/test_documents/case_001" ]; then
          echo "‚úÖ case_001 found"
          ls -la tests/evaluation/test_documents/case_001/
        else
          echo "‚ùå case_001 not found"
        fi
        if [ -d "tests/evaluation/test_documents/case_002" ]; then
          echo "‚úÖ case_002 found"
          ls -la tests/evaluation/test_documents/case_002/
        else
          echo "‚ùå case_002 not found"
        fi

    - name: Set up Azure credentials
      uses: azure/login@v2
      with:
        client-id: ${{ secrets.AZURE_CLIENT_ID }}
        tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

    - name: Run groundedness evaluation
      env:
        LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }}
        LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
        LLM_MODEL_NAME: ${{ secrets.LLM_MODEL_NAME }}
        AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
        AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
        AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        PYTHONPATH: ${{ github.workspace }}/src
      run: |
        cd tests/evaluation
        python groundedness_evaluator.py

    - name: Upload evaluation results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: evaluation-results-${{ github.run_number }}
        path: |
          tests/evaluation/evaluation_results.json
          tests/evaluation/*.log
        retention-days: 30

    - name: Parse evaluation results
      id: parse_results
      if: always()
      run: |
        cd tests/evaluation
        if [ -f "evaluation_results.json" ]; then
          # Extract key metrics from JSON
          TOTAL_CASES=$(python -c "import json; data=json.load(open('evaluation_results.json')); print(data['evaluation_summary']['total_cases'])")
          PASSED_CASES=$(python -c "import json; data=json.load(open('evaluation_results.json')); print(data['evaluation_summary']['passed_cases'])")
          PASS_RATE=$(python -c "import json; data=json.load(open('evaluation_results.json')); print(f\"{data['evaluation_summary']['pass_rate']:.1%}\")")
          AVG_GROUNDEDNESS=$(python -c "import json; data=json.load(open('evaluation_results.json')); print(f\"{data['evaluation_summary']['avg_groundedness_score']:.2f}\")")
          AVG_RELEVANCE=$(python -c "import json; data=json.load(open('evaluation_results.json')); print(f\"{data['evaluation_summary']['avg_relevance_score']:.2f}\")")
          PASSED_THRESHOLD=$(python -c "import json; data=json.load(open('evaluation_results.json')); print(data['evaluation_summary']['passed_threshold'])")
          
          echo "total_cases=$TOTAL_CASES" >> $GITHUB_OUTPUT
          echo "passed_cases=$PASSED_CASES" >> $GITHUB_OUTPUT
          echo "pass_rate=$PASS_RATE" >> $GITHUB_OUTPUT
          echo "avg_groundedness=$AVG_GROUNDEDNESS" >> $GITHUB_OUTPUT
          echo "avg_relevance=$AVG_RELEVANCE" >> $GITHUB_OUTPUT
          echo "passed_threshold=$PASSED_THRESHOLD" >> $GITHUB_OUTPUT
        else
          echo "No evaluation results file found"
          echo "total_cases=0" >> $GITHUB_OUTPUT
          echo "passed_cases=0" >> $GITHUB_OUTPUT
          echo "pass_rate=0%" >> $GITHUB_OUTPUT
          echo "avg_groundedness=0.00" >> $GITHUB_OUTPUT
          echo "avg_relevance=0.00" >> $GITHUB_OUTPUT
          echo "passed_threshold=false" >> $GITHUB_OUTPUT
        fi

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          let comment = `## üîç CAHMS Evaluation Results
          
          **Evaluation Status:** `;
          
          try {
            if (fs.existsSync('tests/evaluation/evaluation_results.json')) {
              const results = JSON.parse(fs.readFileSync('tests/evaluation/evaluation_results.json', 'utf8'));
              const summary = results.evaluation_summary;
              
              const status = summary.passed_threshold ? '‚úÖ **PASSED**' : '‚ùå **FAILED**';
              const passRate = (summary.pass_rate * 100).toFixed(1);
              
              comment += `${status}
              
              | Metric | Value | Threshold |
              |--------|-------|-----------|
              | **Pass Rate** | ${passRate}% (${summary.passed_cases}/${summary.total_cases}) | ${(summary.thresholds.pass_rate_threshold * 100).toFixed(0)}% |
              | **Avg Groundedness** | ${summary.avg_groundedness_score.toFixed(2)}/5.0 | ${summary.thresholds.groundedness_threshold}/5.0 |
              | **Avg Relevance** | ${summary.avg_relevance_score.toFixed(2)}/5.0 | ${summary.thresholds.relevance_threshold}/5.0 |
              | **Avg Generation Time** | ${summary.avg_generation_time_seconds.toFixed(1)}s | - |
              
              <details>
              <summary>üìã Detailed Case Results</summary>
              
              | Case ID | Groundedness | Relevance | Gen Time | Status |
              |---------|--------------|-----------|----------|--------|`;
              
              results.case_results.forEach(r => {
                const status = r.passed_threshold ? '‚úÖ' : '‚ùå';
                comment += `\n| ${r.case_id} | ${r.groundedness_score.toFixed(2)} | ${r.relevance_score.toFixed(2)} | ${r.generation_time_seconds.toFixed(1)}s | ${status} |`;
              });
              
              comment += `\n\n</details>`;
              
              if (results.failed_cases && results.failed_cases.length > 0) {
                comment += `\n\n<details>\n<summary>‚ùå Failed Cases Analysis</summary>\n\n`;
                results.failed_cases.forEach(f => {
                  comment += `**${f.case_id}:**\n`;
                  comment += `- Groundedness: ${f.groundedness_score.toFixed(2)} (threshold: ${summary.thresholds.groundedness_threshold})\n`;
                  comment += `- Relevance: ${f.relevance_score.toFixed(2)} (threshold: ${summary.thresholds.relevance_threshold})\n`;
                  if (f.error_message) {
                    comment += `- Error: ${f.error_message}\n`;
                  }
                  comment += `\n`;
                });
                comment += `</details>`;
              }
              
            } else {
              comment += `‚ùå **FAILED** - No evaluation results generated
              
              The evaluation could not be completed. Please check the workflow logs for details.`;
            }
          } catch (error) {
            comment += `‚ùå **ERROR** - Failed to parse evaluation results
            
            Error: ${error.message}`;
          }
          
          // Add artifact link
          comment += `\n\nüìé **Artifacts:** [Download evaluation results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Set evaluation status
      if: always()
      run: |
        if [ "${{ steps.parse_results.outputs.passed_threshold }}" = "true" ]; then
          echo "‚úÖ Groundedness evaluation PASSED"
          exit 0
        else
          echo "‚ùå Groundedness evaluation FAILED"
          exit 1
        fi

  # TODO: Production - Enable automatic deployment after evaluation passes
  # This job only runs if evaluation passes and we're on main branch
  # trigger-deployment:
  #   runs-on: ubuntu-latest
  #   needs: evaluate
  #   if: github.ref == 'refs/heads/main' && success()
  #   
  #   steps:
  #   - name: Evaluation passed - ready for deployment
  #     run: |
  #       echo "üöÄ Groundedness evaluation passed"
  #       echo "The main deployment workflow can now proceed"
  #       echo "Deployment will be triggered by the existing workflow"
